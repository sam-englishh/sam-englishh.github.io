[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About Me:\nMy name is Sam and I am learning how to code with R and this is my first website! I am majoring in Economics and planning to minor in Data Science at Pomona College. I enjoy working with numbers and data to explain my reasoning and love learning about different ways to present data. Have a look at the data visualizations I made on my website :)\nAbout This Website:\nThis website contains projects that I was tasked to do as a part of my introductory data science course at Pomona College, taught by Johanna Hardin. Also on this website is a presentation that I made to explain the visualizations that I made as a part of each of my projects. You can view all of my work under the “Projects” tab."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Sam English",
    "section": "",
    "text": "Hi! My name is Sam and I am learning how to code with R and this is my first website! I am majoring in Economics and planning to minor in Data Science. I enjoy working with numbers and data to explain my reasoning and love learning about different ways to present data. Have a look at the data visualizations I made on my website :)"
  },
  {
    "objectID": "ChocolateRatings.html",
    "href": "ChocolateRatings.html",
    "title": "Chocolate Ratings",
    "section": "",
    "text": "In the second part of my first project for my introductory data science course, I chose to visualize data that contains information about varying chocolates. The goal of my analysis and visualization is to display the ratings of different chocolates based on the percent of cocoa in them.\nI retrieved this data from the ‘tidytuesday’ data set titled “Chocolate Ratings”:\nhttps://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2022/2022-01-18/chocolate.csv\nThis dataset exists to provide information about various chocolate bars, including their ratings, origin, and cocoa percentages, helping to explore trends in chocolate quality and production.\nI sourced the data for this analysis from TidyTuesday, a project that curates weekly datasets for learning and exploration. The dataset, originally compiled by Georgios and Kelsey and published on the “Flavors of Cocoa” website, offers a detailed look into the world of chocolate ratings and origins.\n\ntuesdata &lt;- tidytuesdayR::tt_load('2022-01-18')\ntuesdata &lt;- tidytuesdayR::tt_load(2022, week = 3)\n\nchocolate &lt;- tuesdata$chocolate\n\nHere is the entire dataset that I used in my visualization.\n\nlibrary(tidyverse)\nchocolate\n\n# A tibble: 2,530 × 10\n     ref company_manufacturer company_location review_date\n   &lt;dbl&gt; &lt;chr&gt;                &lt;chr&gt;                  &lt;dbl&gt;\n 1  2454 5150                 U.S.A.                  2019\n 2  2458 5150                 U.S.A.                  2019\n 3  2454 5150                 U.S.A.                  2019\n 4  2542 5150                 U.S.A.                  2021\n 5  2546 5150                 U.S.A.                  2021\n 6  2546 5150                 U.S.A.                  2021\n 7  2542 5150                 U.S.A.                  2021\n 8   797 A. Morin             France                  2012\n 9   797 A. Morin             France                  2012\n10  1011 A. Morin             France                  2013\n# ℹ 2,520 more rows\n# ℹ 6 more variables: country_of_bean_origin &lt;chr&gt;,\n#   specific_bean_origin_or_bar_name &lt;chr&gt;, cocoa_percent &lt;chr&gt;,\n#   ingredients &lt;chr&gt;, most_memorable_characteristics &lt;chr&gt;, rating &lt;dbl&gt;\n\n\nOnce again, it contains a lot of information, most of it unnecessary for the purposes of my visualization.\nHere is the data that I actually used.\n\nchocolate |&gt;\n  mutate(cocoa_percent = as.numeric(sub(\"%\", \"\", cocoa_percent))) |&gt;\n  select(cocoa_percent, rating) |&gt;\n  group_by(cocoa_percent) |&gt;\n  arrange(cocoa_percent) |&gt;\n  summarise(ave_rating = mean(rating))\n\n# A tibble: 46 × 2\n   cocoa_percent ave_rating\n           &lt;dbl&gt;      &lt;dbl&gt;\n 1          42         2.75\n 2          46         2.75\n 3          50         3.75\n 4          53         2   \n 5          55         2.86\n 6          56         3.25\n 7          57         2.75\n 8          58         3.12\n 9          60         3.01\n10          60.5       2.75\n# ℹ 36 more rows\n\n\nBelow is a graph that shows the relationship between the percent of cocoa in a chocolate bar and how it is rated on a scale of 1 to 5.\n\nsummary_data &lt;- chocolate |&gt;\n  mutate(cocoa_percent = as.numeric(sub(\"%\", \"\", cocoa_percent))) |&gt;\n  select(cocoa_percent, rating) |&gt;\n  group_by(cocoa_percent) |&gt;\n  arrange(cocoa_percent) |&gt;\n  summarise(ave_rating = mean(rating))\n\n\nggplot(summary_data, aes(x = cocoa_percent, y = ave_rating)) +\n  geom_point() +\n  geom_smooth(se = FALSE) +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +\n  labs(\n    x = \"Percent of Cocoa in Chocolate Bar\",\n    y = \"Rating\",\n    title = \"Rating of Chocolate Bar by Percent Cocoa\"\n  )\n\n\n\n\n\n\n\n\nI really enjoyed making this visualization as I think it actually answers a question that I find interesting: “What is the ideal percent of cocoa in a chocolate bar?” The graph demonstrates a concave down curve, with a rough maximum at 68% cocoa. It is super cool to answer a question like this in my first project involving r. It also interesting to note that, as expected, the results aren’t linear or smooth. This suggests that maybe there are certain levels of cocoa percent that match flavor expectations. A good example of this is the data point at 50% cocoa, which is way higher than the points either side of it. Maybe exactly 50% cocoa is ideal, but 45% and 55% have an unappeasing taste."
  },
  {
    "objectID": "WorldCup.html",
    "href": "WorldCup.html",
    "title": "World Cup",
    "section": "",
    "text": "library(tidyverse)\n\nIn this project, my first time using the data visualization tools learnt in my introductory data science course, I aimed to represent the increase in the number of goals scored at each soccer (football) World Cup between the first in 1930 and the one held in Russia in 2018.\nI retrieved this data from the ‘tidytuesday’ data set titled “World Cup”:\nhttps://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2022/2022-11-29/worldcups.csv\nThe purpose of the data set was to show information about all of the World Cups before the start of the next in 2022, which was to be hosted in Qatar starting on 11/20/2022.\nI sourced the data for this analysis from TidyTuesday, a project that curates weekly datasets for learning and exploration. The dataset, originally created by Evan Gower and published on Kaggle, focuses on FIFA World Cup statistics.\nHere is the data set that I used to present my findings.\n\nworldcups &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2022/2022-11-29/worldcups.csv')\nworldcups\n\n# A tibble: 21 × 10\n    year host     winner second third fourth goals_scored teams games attendance\n   &lt;dbl&gt; &lt;chr&gt;    &lt;chr&gt;  &lt;chr&gt;  &lt;chr&gt; &lt;chr&gt;         &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;      &lt;dbl&gt;\n 1  1930 Uruguay  Urugu… Argen… USA   Yugos…           70    13    18     434000\n 2  1934 Italy    Italy  Czech… Germ… Austr…           70    16    17     395000\n 3  1938 France   Italy  Hunga… Braz… Sweden           84    15    18     483000\n 4  1950 Brazil   Urugu… Brazil Swed… Spain            88    13    22    1337000\n 5  1954 Switzer… West … Hunga… Aust… Urugu…          140    16    26     943000\n 6  1958 Sweden   Brazil Sweden Fran… West …          126    16    35     868000\n 7  1962 Chile    Brazil Czech… Chile Yugos…           89    16    32     776000\n 8  1966 England  Engla… West … Port… Sovie…           89    16    32    1614677\n 9  1970 Mexico   Brazil Italy  West… Urugu…           95    16    32    1673975\n10  1974 Germany  West … Nethe… Pola… Brazil           97    16    38    1774022\n# ℹ 11 more rows\n\n\nThis table contains information about the results, host country, and matches played. This information isn’t necessary for my visualization.\nHere is the data that I actually used in my visualization:\n\nworldcups |&gt;\n  group_by(year) |&gt;\n  select(goals_scored)\n\n# A tibble: 21 × 2\n# Groups:   year [21]\n    year goals_scored\n   &lt;dbl&gt;        &lt;dbl&gt;\n 1  1930           70\n 2  1934           70\n 3  1938           84\n 4  1950           88\n 5  1954          140\n 6  1958          126\n 7  1962           89\n 8  1966           89\n 9  1970           95\n10  1974           97\n# ℹ 11 more rows\n\n\nBelow is a graph that shows the number of goals scored at each world cup.\n\nworldcups &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2022/2022-11-29/worldcups.csv')\n\nggplot(worldcups, aes(x = year, y = goals_scored)) + \n  geom_point() + \n  geom_smooth(se = FALSE) +\n  labs(\n    x = \"World Cup Year\",\n    y = \"Number of Goals Scored\",\n    title = \"Number of Goals Scored Per World Cup\"\n  )"
  },
  {
    "objectID": "Mini Project 2.html",
    "href": "Mini Project 2.html",
    "title": "Mini Project 2",
    "section": "",
    "text": "This analysis uses data from the tidy tuesday data source on titles of Netflix movies and TV show. Netflix Titles\n\ntuesdata &lt;- tidytuesdayR::tt_load('2021-04-20')\ntuesdata &lt;- tidytuesdayR::tt_load(2021, week = 17)\n\nnetflix &lt;- tuesdata$netflix\n\nnetflix\n\n# A tibble: 7,787 × 12\n   show_id type    title director   cast  country date_added release_year rating\n   &lt;chr&gt;   &lt;chr&gt;   &lt;chr&gt; &lt;chr&gt;      &lt;chr&gt; &lt;chr&gt;   &lt;chr&gt;             &lt;dbl&gt; &lt;chr&gt; \n 1 s1      TV Show 3%    &lt;NA&gt;       João… Brazil  August 14…         2020 TV-MA \n 2 s2      Movie   7:19  Jorge Mic… Demi… Mexico  December …         2016 TV-MA \n 3 s3      Movie   23:59 Gilbert C… Tedd… Singap… December …         2011 R     \n 4 s4      Movie   9     Shane Ack… Elij… United… November …         2009 PG-13 \n 5 s5      Movie   21    Robert Lu… Jim … United… January 1…         2008 PG-13 \n 6 s6      TV Show 46    Serdar Ak… Erda… Turkey  July 1, 2…         2016 TV-MA \n 7 s7      Movie   122   Yasir Al … Amin… Egypt   June 1, 2…         2019 TV-MA \n 8 s8      Movie   187   Kevin Rey… Samu… United… November …         1997 R     \n 9 s9      Movie   706   Shravan K… Divy… India   April 1, …         2019 TV-14 \n10 s10     Movie   1920  Vikram Bh… Rajn… India   December …         2008 TV-MA \n# ℹ 7,777 more rows\n# ℹ 3 more variables: duration &lt;chr&gt;, listed_in &lt;chr&gt;, description &lt;chr&gt;\n\n\n\nlibrary(dplyr)\nlibrary(tidytext)\nlibrary(tidyr)\nlibrary(stringr)\nlibrary(ggplot2)\n\n\nstop_words &lt;- c(\"the\", \"a\", \"of\", \"and\", \"in\", \"on\", \"with\", \"is\", \"to\", \"for\", \"at\", \"s\", \"2\", \"i\")\n\ncommon_words &lt;- netflix |&gt;\n  mutate(title = str_to_lower(title),                               \n         title = str_replace_all(title, \"[[:punct:]]\", \" \")) |&gt;     \n  rowwise() |&gt;                                                      \n  mutate(words = str_split(title, \"\\\\s+\")) |&gt;                       \n  unnest(words) |&gt;                                                  \n  filter(!words %in% stop_words,                                     \n         words != \"\") |&gt;                                            \n  group_by(words) |&gt;                                      \n  mutate(n = n()) |&gt;                                            \n  arrange(desc(n)) |&gt;                                          \n  distinct(words, n, .keep_all = TRUE) |&gt;\n  select(words, n) |&gt;\n  head(10)\n\ncommon_words\n\n# A tibble: 10 × 2\n# Groups:   words [10]\n   words         n\n   &lt;chr&gt;     &lt;int&gt;\n 1 love        152\n 2 my          127\n 3 you          81\n 4 man          79\n 5 christmas    78\n 6 world        69\n 7 story        67\n 8 life         66\n 9 movie        60\n10 little       58\n\n\n\nggplot(common_words, aes(x = reorder(words, -n),  y = n)) + \n  geom_bar(stat = \"identity\", fill = \"pink\") +\n  labs(title = \"Most Common Words in Netflix Titles\", \n       x = \"Words\", \n       y = \"Frequency\") +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1)) \n\n\n\n\n\n\n\n\nThis plot shows the most common occurring words in Netflix titles, if you remove stock words such as “the”, “and” and “of”. Some obvious ‘buzzwords’ include “love,”story” and “movie”. Some key words that surprised me include “christmas”, but it also makes sense as Christmas movies, much like songs, are very popular at that specific time of year. “Man” is also an interesting observation considering there is no appearance of “woman” in the top 10 words.\n\nnetflix_by_year &lt;- netflix |&gt;\n  group_by(release_year, type) |&gt;\n  summarise(count = n())  \n\nnetflix_by_year\n\n# A tibble: 118 × 3\n# Groups:   release_year [73]\n   release_year type    count\n          &lt;dbl&gt; &lt;chr&gt;   &lt;int&gt;\n 1         1925 TV Show     1\n 2         1942 Movie       2\n 3         1943 Movie       3\n 4         1944 Movie       3\n 5         1945 Movie       3\n 6         1946 Movie       1\n 7         1946 TV Show     1\n 8         1947 Movie       1\n 9         1954 Movie       2\n10         1955 Movie       3\n# ℹ 108 more rows\n\nggplot(netflix_by_year, aes(x = release_year, y = count, color = type)) +\n  geom_line() +\n  labs(title = \"Number of Movies and TV Shows Released by Year on Netflix\", \n       x = \"Release Year\", \n       y = \"Count\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nThis graph shows the number of movies and TV shows and their corresponding release years. It shows the increase in number of movies as the year heads towards present day. It also demonstrates the disparity of movies vs. TV shows. And that there has always been more movies than TT shows on Netflix. An interesting observation is in 2021, where there is growth in the number of TV shows released in this year, and a decline in movies. It is also cool to note that the earliest title on Netflix was released in 1925, and it is a TV show!!\n\ntotal_titles &lt;- nrow(netflix)\n\nprop_titles &lt;- netflix |&gt;\n  mutate(title = str_to_lower(title)) |&gt; \n  summarise(\n    count_numbers = sum(str_detect(title, \"\\\\d\")),  \n    count_the_anywhere = sum(str_detect(title, \"the\")),  \n    count_the_start = sum(str_detect(title, \"^the\"))  \n  ) |&gt;\n  mutate(\n    The_Anywhere = count_the_anywhere / total_titles, \n    Start_With_The = count_the_start / total_titles, \n    Digit_Anywhere = count_numbers / total_titles  \n  )\n\nprop_titles\n\n# A tibble: 1 × 6\n  count_numbers count_the_anywhere count_the_start The_Anywhere Start_With_The\n          &lt;int&gt;              &lt;int&gt;           &lt;int&gt;        &lt;dbl&gt;          &lt;dbl&gt;\n1           426               1898             997        0.244          0.128\n# ℹ 1 more variable: Digit_Anywhere &lt;dbl&gt;\n\nprop_titles_long &lt;- prop_titles |&gt;\n  pivot_longer(\n    cols = c(The_Anywhere, Start_With_The, Digit_Anywhere),  \n    names_to = \"condition\",              \n    values_to = \"proportion\"             \n  ) |&gt;\n  select(condition, proportion)\n\nprop_titles_long \n\n# A tibble: 3 × 2\n  condition      proportion\n  &lt;chr&gt;               &lt;dbl&gt;\n1 The_Anywhere       0.244 \n2 Start_With_The     0.128 \n3 Digit_Anywhere     0.0547\n\nggplot(prop_titles_long, aes(x = condition, y = proportion, fill = condition)) +\n  geom_bar(stat = \"identity\") +\n  labs(\n    title = \"Proportions of Titles in Netflix\",\n    x = \"Title Condition\",\n    y = \"Proportion\",\n    fill = \"Condition\"\n  ) +\n  scale_y_continuous(labels = scales::percent) +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n\n\n\n\n\n\n\nThis plot shows the proportions of titles that start with “the”, contain “the” or contain any digit. Almost exactly twice as many titles contain “the” anywhere in the title than titles that start with “the”. Almost a quarter of Netflix titles contain the word “the”. Far fewer titles contain digits, at only 5.5%."
  },
  {
    "objectID": "MiniProject3.html",
    "href": "MiniProject3.html",
    "title": "Mini Project 3",
    "section": "",
    "text": "Research Question: Is there a significant relationship between parents using marijuana and their children’s marijuana usage?\nIn this project, I plan to investigate the potential relationship between parental and child marijuana use using a permutation test. First, I will calculate the observed difference in the rate of marijuana use among children based on whether their parents also use marijuana. Then, I’ll perform a permutation test by shuffling the parental usage labels multiple times (1,000 permutations) to simulate the distribution of differences under the null hypothesis of no relationship. Finally, I’ll compare the observed difference to this simulated distribution to determine if the observed association is statistically significant, and I’ll visualize the results to support my findings.\n\nlibrary(openintro)\ndata(drug_use)\n\n\nstr(drug_use)\n\ntibble [445 × 2] (S3: tbl_df/tbl/data.frame)\n $ student: Factor w/ 2 levels \"not\",\"uses\": 2 2 2 2 2 2 2 2 2 2 ...\n $ parents: Factor w/ 2 levels \"not\",\"used\": 2 2 2 2 2 2 2 2 2 2 ...\n\nsummary(drug_use)\n\n student    parents   \n not :226   not :235  \n uses:219   used:210  \n\n\n\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nobserved_data &lt;- drug_use |&gt;\n  group_by(parents) |&gt;\n  summarise(child_use_avg = mean(student == \"uses\")) |&gt;\n  summarise(difference = diff(child_use_avg)) |&gt;\n  pull(difference)\nobserved_data\n\n[1] 0.1952381\n\n\n\nlibrary(purrr)\nperm_test &lt;- function(data) {\n  data |&gt;\n    mutate(student_samp = sample(student, replace = FALSE)) |&gt;\n    group_by(parents) |&gt;\n    summarise(child_use_avg = mean(student_samp == \"uses\")) |&gt;\n    summarise(difference = diff(child_use_avg)) |&gt;\n    pull(difference)\n}\nn_perm &lt;- 1000\n\nperm_results &lt;- map_dbl(1:n_perm, ~perm_test(drug_use))\n\n\nlibrary(ggplot2)\n\nggplot(data.frame(perm_results), aes(x = perm_results)) +\n  geom_histogram(bins = 30, color = \"black\", fill = \"skyblue\") +\n  geom_vline(xintercept = observed_data, color = \"red\") +\n  labs(title = \"Permutation Test: Difference in Child Marijuana Use Rates\",\n       x = \"Difference in Rates\", y = \"Frequency\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\np_value &lt;- mean(perm_results &gt;= observed_data)\np_value\n\n[1] 0\n\n\nIn this analysis, I investigated the association between parental and child marijuana use using a permutation test. I first calculated the observed difference in child marijuana use rates based on whether parents used marijuana. After running 1,000 permutations to generate a distribution of differences under the null hypothesis (assuming no relationship between parental and child use), I found a p-value of 0. This indicates that none of the randomly shuffled differences were as extreme as the observed difference, suggesting a very strong association between parental and child marijuana use that is unlikely to have occurred by chance.\nIt’s important to note that the dataset’s representativeness is uncertain; we don’t know the source population or whether the sample accurately reflects broader demographics. As such, these results might not generalize to the global population, as factors such as cultural context and socioeconomic status could influence these behaviors in ways not captured here."
  },
  {
    "objectID": "MiniProject4.html",
    "href": "MiniProject4.html",
    "title": "SQL Analysis of WAI Data for Auditory Research",
    "section": "",
    "text": "In this analysis, I aim to explore the Wideband Acoustic Immittance (WAI) database by recreating a plot of mean absorbance versus frequency for studies in the database. First, I will extract the data needed to recreate a plot similar to the one in Voss, including mean absorbance for each study, the number of unique ears, and the instruments used. Then, I will identify a study with participants from diverse demographic groups (e.g., males and females) and analyze differences in mean absorbance between these groups. This will involve grouping the data by study, demographic groups, and frequency and visualizing the trends.\nThe data used in this analysis was obtained from the [Wideband Acoustic Immittance (WAI) Database] hosted by Smith College. The WAI database compiles published WAI measurements, which serve as noninvasive auditory diagnostic tools. These measurements include data from various peer-reviewed studies, and details about the database are available in the publication by Voss et al. (2019).\nThe analysis reproduces Figure 1 from the article: Voss, Susan E. Ph.D. (2019). Resource Review. Ear and Hearing, 40(6), p. 1481, November/December 2019. DOI: 10.1097/AUD.0000000000000790\nHere is the source for the 2004 study conducted by Abur, which I refer to in the second part of my project. Abur, D., Horton, N. J., & Voss, S. E. (2014).** Wideband acoustic immittance measures in normal-hearing adults: Test-retest reliability and effects of ear-canal pressurization. Hearing Research, 316, 23–31. DOI Link\nHere are the three datasets that I use in my project.\n\nlibrary(tidyverse)\nlibrary(RMariaDB)\ncon_wai &lt;- dbConnect(\n  MariaDB(), host = \"scidb.smith.edu\",\n  user = \"waiuser\", password = \"smith_waiDB\", \n  dbname = \"wai\"\n)\nMeasurements &lt;- tbl(con_wai, \"Measurements\")\nPI_Info &lt;- tbl(con_wai, \"PI_Info\")\nSubjects &lt;- tbl(con_wai, \"Subjects\")\n\nMeasurements\n\n# Source:   table&lt;`Measurements`&gt; [?? x 16]\n# Database: mysql  [waiuser@scidb.smith.edu:3306/wai]\n   Identifier SubjectNumber Session Ear   Instrument   Age AgeCategory EarStatus\n   &lt;chr&gt;              &lt;int&gt;   &lt;int&gt; &lt;chr&gt; &lt;chr&gt;      &lt;dbl&gt; &lt;chr&gt;       &lt;chr&gt;    \n 1 Abur_2014              1       1 Left  HearID        20 Adult       Normal   \n 2 Abur_2014              1       1 Left  HearID        20 Adult       Normal   \n 3 Abur_2014              1       1 Left  HearID        20 Adult       Normal   \n 4 Abur_2014              1       1 Left  HearID        20 Adult       Normal   \n 5 Abur_2014              1       1 Left  HearID        20 Adult       Normal   \n 6 Abur_2014              1       1 Left  HearID        20 Adult       Normal   \n 7 Abur_2014              1       1 Left  HearID        20 Adult       Normal   \n 8 Abur_2014              1       1 Left  HearID        20 Adult       Normal   \n 9 Abur_2014              1       1 Left  HearID        20 Adult       Normal   \n10 Abur_2014              1       1 Left  HearID        20 Adult       Normal   \n# ℹ more rows\n# ℹ 8 more variables: TPP &lt;dbl&gt;, AreaCanal &lt;dbl&gt;, PressureCanal &lt;dbl&gt;,\n#   SweepDirection &lt;chr&gt;, Frequency &lt;dbl&gt;, Absorbance &lt;dbl&gt;, Zmag &lt;dbl&gt;,\n#   Zang &lt;dbl&gt;\n\nPI_Info\n\n# Source:   table&lt;`PI_Info`&gt; [?? x 12]\n# Database: mysql  [waiuser@scidb.smith.edu:3306/wai]\n   Identifier    Year Authors      AuthorsShortList Title Journal URL   Abstract\n   &lt;chr&gt;        &lt;int&gt; &lt;chr&gt;        &lt;chr&gt;            &lt;chr&gt; &lt;chr&gt;   &lt;chr&gt; &lt;chr&gt;   \n 1 Abur_2014     2014 Defne Abur,… Abur et al.      Inst… J Am A… http… \"\\\"&lt;p&gt; …\n 2 Aithal_2013   2013 Sreedevi Ai… Aithal et al.    Norm… Int. J… http… \"&lt;p&gt; &lt;s…\n 3 Aithal_2014   2014 Sreedevi Ai… Aithal et al.    Wide… J Am A… http… \"&lt;p&gt; &lt;s…\n 4 Aithal_2014b  2014 Sreedevi Ai… Aithal et al.    Wide… J Am A… http… \"&lt;p&gt; &lt;s…\n 5 Aithal_2015   2015 Sreedevi Ai… Aithal et al.    Wide… Ear He… http… \"&lt;p&gt; &lt;s…\n 6 Aithal_2017a  2017 Sreedevi Ai… Aithal et al.    Norm… J Spee… http… \"&lt;p&gt; &lt;s…\n 7 Aithal_2017b  2017 Sreedevi Ai… Aithal et al.    Effe… Int J … http… \"&lt;p&gt; &lt;s…\n 8 Aithal_2019a  2019 Sreedevi Ai… Aithal et al.    Eust… J Am A… http… \"&lt;p&gt; &lt;s…\n 9 Aithal_2019b  2019 Sreedevi Ai… Aithal et al.    Effe… J Spee… http… \"Object…\n10 Aithal_2020a  2020 Sreedevi Ai… Aithal et al.    Pred… J Am A… http… \"&lt;p&gt; &lt;s…\n# ℹ more rows\n# ℹ 4 more variables: DataSubmitterName &lt;chr&gt;, DataSubmitterEmail &lt;chr&gt;,\n#   DateSubmitted &lt;chr&gt;, PI_Notes &lt;chr&gt;\n\nSubjects\n\n# Source:   table&lt;`Subjects`&gt; [?? x 11]\n# Database: mysql  [waiuser@scidb.smith.edu:3306/wai]\n   Identifier  SubjectNumber SessionTotal AgeFirstMeasurement\n   &lt;chr&gt;               &lt;int&gt;        &lt;int&gt;               &lt;dbl&gt;\n 1 Abur_2014               1            7            20      \n 2 Abur_2014               3            8            19      \n 3 Abur_2014               4            7            21      \n 4 Abur_2014               6            8            21      \n 5 Abur_2014               7            5            20      \n 6 Abur_2014               8            5            19      \n 7 Abur_2014              10            5            19      \n 8 Aithal_2013             1            1            NA      \n 9 Aithal_2013             2            1             0.00744\n10 Aithal_2013             3            1            NA      \n# ℹ more rows\n# ℹ 7 more variables: AgeCategoryFirstMeasurement &lt;chr&gt;, Sex &lt;chr&gt;, Race &lt;chr&gt;,\n#   Ethnicity &lt;chr&gt;, LeftEarStatusFirstMeasurement &lt;chr&gt;,\n#   RightEarStatusFirstMeasurement &lt;chr&gt;, SubjectNotes &lt;chr&gt;\n\n\nHere I find all of the different names of tables available to access.\n\nSHOW TABLES;\n\n\n7 records\n\n\nTables_in_wai\n\n\n\n\nCodebook\n\n\nMeasurements\n\n\nMeasurements_pre2020\n\n\nPI_Info\n\n\nPI_Info_OLD\n\n\nSubjects\n\n\nSubjects_pre2020\n\n\n\n\n\nThis line of code describes each table.\n\nDESCRIBE Measurements;\n\n\nDisplaying records 1 - 10\n\n\nField\nType\nNull\nKey\nDefault\nExtra\n\n\n\n\nIdentifier\nvarchar(50)\nNO\nPRI\nNA\n\n\n\nSubjectNumber\nint\nNO\nPRI\nNA\n\n\n\nSession\nint\nNO\nPRI\nNA\n\n\n\nEar\nvarchar(50)\nNO\nPRI\n\n\n\n\nInstrument\nvarchar(50)\nNO\nPRI\n\n\n\n\nAge\nfloat\nYES\n\nNA\n\n\n\nAgeCategory\nvarchar(50)\nYES\n\nNA\n\n\n\nEarStatus\nvarchar(50)\nYES\n\nNA\n\n\n\nTPP\nfloat\nYES\n\nNA\n\n\n\nAreaCanal\nfloat\nYES\n\nNA\n\n\n\n\n\n\nThis shows the first 5 rows of the Measurements table.\n\nSELECT *\nFROM Measurements\nLIMIT 0, 5;\n\n\n5 records\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIdentifier\nSubjectNumber\nSession\nEar\nInstrument\nAge\nAgeCategory\nEarStatus\nTPP\nAreaCanal\nPressureCanal\nSweepDirection\nFrequency\nAbsorbance\nZmag\nZang\n\n\n\n\nAbur_2014\n1\n1\nLeft\nHearID\n20\nAdult\nNormal\n-5\n4.42e-05\n0\nAmbient\n210.938\n0.0333379\n113780000\n-0.233504\n\n\nAbur_2014\n1\n1\nLeft\nHearID\n20\nAdult\nNormal\n-5\n4.42e-05\n0\nAmbient\n234.375\n0.0315705\n103585000\n-0.235778\n\n\nAbur_2014\n1\n1\nLeft\nHearID\n20\nAdult\nNormal\n-5\n4.42e-05\n0\nAmbient\n257.812\n0.0405751\n92951696\n-0.233482\n\n\nAbur_2014\n1\n1\nLeft\nHearID\n20\nAdult\nNormal\n-5\n4.42e-05\n0\nAmbient\n281.250\n0.0438399\n86058000\n-0.233421\n\n\nAbur_2014\n1\n1\nLeft\nHearID\n20\nAdult\nNormal\n-5\n4.42e-05\n0\nAmbient\n304.688\n0.0486400\n79492800\n-0.232931\n\n\n\n\n\nThen, I selected the data that I wanted to use for my visualization. This code block extracts the information.\n\nSELECT \n  Measurements.Identifier,\n  COUNT(DISTINCT CONCAT(Measurements.SubjectNumber, Measurements.Ear)) AS Unique_Ears,\n  PI_Info.AuthorsShortList,\n  Measurements.Instrument,\n  Measurements.Frequency,\n  AVG(Measurements.Absorbance) AS MeanAbsorbance,\n  CONCAT(PI_Info.AuthorsShortList, ' et al. N=', \n         COUNT(DISTINCT CONCAT(Measurements.SubjectNumber, Measurements.Ear)), ', ', Measurements.Instrument) AS LegendLabel\nFROM Measurements\nJOIN PI_Info ON Measurements.Identifier = PI_Info.Identifier\nWHERE Measurements.Identifier IN ('Abur_2014', 'Feeney_207', 'Groon_2015', 'Lewis_2015', 'Liu_2008', 'Rosowski_2012', 'Shahnaz_2006', 'Shaver_2013', 'Sun_2016', 'Voss_1994', 'Voss_2010', 'Werner_2010')\nGROUP BY Measurements.Identifier, Measurements.Instrument, PI_Info.AuthorsShortList, Measurements.Frequency;\n\n\nDisplaying records 1 - 10\n\n\n\n\n\n\n\n\n\n\n\nIdentifier\nUnique_Ears\nAuthorsShortList\nInstrument\nFrequency\nMeanAbsorbance\nLegendLabel\n\n\n\n\nAbur_2014\n14\nAbur et al.\nHearID\n210.938\n0.0784746\nAbur et al. et al. N=14, HearID\n\n\nAbur_2014\n14\nAbur et al.\nHearID\n234.375\n0.0826420\nAbur et al. et al. N=14, HearID\n\n\nAbur_2014\n14\nAbur et al.\nHearID\n257.812\n0.0948482\nAbur et al. et al. N=14, HearID\n\n\nAbur_2014\n14\nAbur et al.\nHearID\n281.250\n0.1031472\nAbur et al. et al. N=14, HearID\n\n\nAbur_2014\n14\nAbur et al.\nHearID\n304.688\n0.1137576\nAbur et al. et al. N=14, HearID\n\n\nAbur_2014\n14\nAbur et al.\nHearID\n328.125\n0.1221205\nAbur et al. et al. N=14, HearID\n\n\nAbur_2014\n14\nAbur et al.\nHearID\n351.562\n0.1334329\nAbur et al. et al. N=14, HearID\n\n\nAbur_2014\n14\nAbur et al.\nHearID\n375.000\n0.1447725\nAbur et al. et al. N=14, HearID\n\n\nAbur_2014\n14\nAbur et al.\nHearID\n398.438\n0.1563874\nAbur et al. et al. N=14, HearID\n\n\nAbur_2014\n14\nAbur et al.\nHearID\n421.875\n0.1806973\nAbur et al. et al. N=14, HearID\n\n\n\n\n\nIn this code block, I output the data that I want to use with the name “data” so I can use ggplot in R to visualize it.\n\nSELECT \n  Measurements.Identifier,\n  COUNT(DISTINCT CONCAT(Measurements.SubjectNumber, Measurements.Ear)) AS Unique_Ears,\n  PI_Info.AuthorsShortList,\n  Measurements.Instrument,\n  Measurements.Frequency,\n  AVG(Measurements.Absorbance) AS MeanAbsorbance,\n  CONCAT(PI_Info.AuthorsShortList, ' et al. N=', \n         COUNT(DISTINCT CONCAT(Measurements.SubjectNumber, Measurements.Ear)), ', ', Measurements.Instrument) AS LegendLabel\nFROM Measurements\nJOIN PI_Info ON Measurements.Identifier = PI_Info.Identifier\nWHERE Measurements.Identifier IN ('Abur_2014', 'Feeney_207', 'Groon_2015', 'Lewis_2015', 'Liu_2008', 'Rosowski_2012', 'Shahnaz_2006', 'Shaver_2013', 'Sun_2016', 'Voss_1994', 'Voss_2010', 'Werner_2010')\n  AND Measurements.Frequency &gt;= 200\nGROUP BY Measurements.Identifier, Measurements.Instrument, PI_Info.AuthorsShortList, Measurements.Frequency;\n\n\nggplot(data, aes(x = Frequency, y = MeanAbsorbance, color = LegendLabel)) +\n  geom_line(size = 0.8) +\n  labs(\n    title = \"Mean absorbance from each publication in WAI database\",\n    x = \"Frequency (Hz)\",\n    y = \"Mean Absorbance\",\n    color = NULL\n  ) +\n  theme_minimal() +\n  scale_x_continuous(\n    trans = \"log10\",\n    breaks = c(200, 400, 600, 800, 1000, 2000, 4000, 6000, 8000),\n    labels = c(\"200\", \"400\", \"600\", \"800\", \"1000\", \"2000\", \"4000\", \"6000\", \"8000\"),\n    limits = c(200, 8000)\n  ) +\n  scale_y_continuous(\n    limits = c(0, 1),\n    expand = c(0, 0)\n  ) +\n  theme(\n    legend.position = c(0.2, 0.8),  # Place legend inside the plot\n    legend.title = element_blank()  # Remove legend title\n  )\n\n\n\n\n\n\n\n\nThis plot shows the mean absorbance at various frequencies for multiple studies in the WAI database. Each line represents a study, and the legend provides detailed information about the study, including the author, number of unique ears included (N), and the instrument used. The x-axis is scaled logarithmically to reflect the wide range of tested frequencies, while the y-axis displays mean absorbance, ranging from 0 to 1. This plot allows for a comparison of absorbance patterns across studies, highlighting similarities and subtle differences in how absorbance varies with frequency. The visualisation looks identical to the graph found online. Although I tried my best to recreate the legend to be exactly the same as online, this was as close as I could get.\n\nSELECT \n  Subjects.Sex, \n  Measurements.Frequency, \n  AVG(Measurements.Absorbance) AS MeanAbsorbance\nFROM Measurements\nJOIN Subjects ON Measurements.SubjectNumber = Subjects.SubjectNumber\nWHERE Measurements.Identifier = 'Abur_2014'\nGROUP BY Subjects.Sex, Measurements.Frequency;\n\n\nDisplaying records 1 - 10\n\n\nSex\nFrequency\nMeanAbsorbance\n\n\n\n\nFemale\n210.938\n0.0795249\n\n\nFemale\n234.375\n0.0837334\n\n\nFemale\n257.812\n0.0958535\n\n\nFemale\n281.250\n0.1042761\n\n\nFemale\n304.688\n0.1148410\n\n\nFemale\n328.125\n0.1233041\n\n\nFemale\n351.562\n0.1344986\n\n\nFemale\n375.000\n0.1458314\n\n\nFemale\n398.438\n0.1573792\n\n\nFemale\n421.875\n0.1814149\n\n\n\n\n\n\nSELECT \n  Subjects.Sex, \n  Measurements.Frequency, \n  AVG(Measurements.Absorbance) AS MeanAbsorbance\nFROM Measurements\nJOIN Subjects ON Measurements.SubjectNumber = Subjects.SubjectNumber\nWHERE Measurements.Identifier = 'Abur_2014'\nGROUP BY Subjects.Sex, Measurements.Frequency;\n\n\nggplot(second_data, aes(x = Frequency, y = MeanAbsorbance, color = Sex)) +\n  geom_line(size = 1) +\n  labs(\n    title = \"Frequency vs. Mean Absorbance by Sex (Abur_2014)\",\n    x = \"Frequency (Hz)\",\n    y = \"Mean Absorbance\",\n    color = \"Sex\"\n  ) +\n  theme_minimal() +\n  scale_x_continuous(\n    trans = \"log10\",\n    breaks = c(200, 400, 600, 800, 1000, 2000, 4000, 6000, 8000),\n    labels = c(\"200\", \"400\", \"600\", \"800\", \"1000\", \"2000\", \"4000\", \"6000\", \"8000\"),\n    limits = c(200, 8000)\n  ) +\n  scale_y_continuous(\n    limits = c(0, 1),\n    expand = c(0, 0)\n  ) +\n  theme(\n    legend.position = \"right\",\n    plot.title = element_text(hjust = 0.5)\n  )\n\n\n\n\n\n\n\n\nThis plot focuses on the Abur_2014 study and compares mean absorbance across frequencies for people who identify as males, females or unknown sexes. Three distinct lines represent the groups, with each point indicating the mean absorbance for a specific frequency. The x-axis is scaled logarithmically, and the y-axis shows absorbance values. The plot reveals that the patterns in mean absorbance by men and women across Abur’s 2014 study are very similar. The line that strays from the common relationship is the data recording people where their sex was unkown. This could be due to far fewer data points, and thus a mean that is more likely to deviate from the population mean.\nIn this project, I recreated a plot showing mean absorbance versus frequency for multiple studies in the WAI database, enriching the legend with information about the number of unique ears and instruments used in each study. The plot highlights the similarities and differences in absorbance patterns across studies while maintaining consistent formatting with prior work. Additionally, I analyzed data from the Abur_2014 study to explore differences in mean absorbance between males, females and people with an unknown sex. By plotting frequency versus mean absorbance for each group, there was seemingly no difference between womens’ and mens’ mean absorbance along the different frequencies."
  },
  {
    "objectID": "MiniProject2.html",
    "href": "MiniProject2.html",
    "title": "Netflix Title Analysis",
    "section": "",
    "text": "This is my second project using R to visualize data. The focus in this project is to use piping to organize data so it is easier to understand and present. I focused on data involving Netflix titles and made three visualizations. Firstly, I found the most common words in titles of Netflix Movies and TV Shows, removing filler words like ‘the’ and ‘and’. Next, I compared the number of titles on Netflix that were movies or TV Shows, and their release dates. Finally, I displayed the percent of titles that contain a digit anywhere in the title, start with “the” or have the word “the” anywhere and showed this information on a bar graph.\nThis analysis uses data from the tidy tuesday data source on titles of Netflix movies and TV show. [Netflix Titles] https://github.com/rfordatascience/tidytuesday/blob/master/data/2021/2021-04-20/readme.md\nthe original data can be found on Kaggle at https://www.kaggle.com/datasets/shivamb/netflix-shows. It was published by Bansal. The data is titled “Netflix Movies and TV Shows”, and consists of listings of all the movies and tv shows available on Netflix, along with details such as - cast, directors, ratings, release year, duration, etc.\nHere is the entire dataset that I used.\n\ntuesdata &lt;- tidytuesdayR::tt_load('2021-04-20')\ntuesdata &lt;- tidytuesdayR::tt_load(2021, week = 17)\nnetflix &lt;- tuesdata$netflix\nnetflix\n\n# A tibble: 7,787 × 12\n   show_id type    title director   cast  country date_added release_year rating\n   &lt;chr&gt;   &lt;chr&gt;   &lt;chr&gt; &lt;chr&gt;      &lt;chr&gt; &lt;chr&gt;   &lt;chr&gt;             &lt;dbl&gt; &lt;chr&gt; \n 1 s1      TV Show 3%    &lt;NA&gt;       João… Brazil  August 14…         2020 TV-MA \n 2 s2      Movie   7:19  Jorge Mic… Demi… Mexico  December …         2016 TV-MA \n 3 s3      Movie   23:59 Gilbert C… Tedd… Singap… December …         2011 R     \n 4 s4      Movie   9     Shane Ack… Elij… United… November …         2009 PG-13 \n 5 s5      Movie   21    Robert Lu… Jim … United… January 1…         2008 PG-13 \n 6 s6      TV Show 46    Serdar Ak… Erda… Turkey  July 1, 2…         2016 TV-MA \n 7 s7      Movie   122   Yasir Al … Amin… Egypt   June 1, 2…         2019 TV-MA \n 8 s8      Movie   187   Kevin Rey… Samu… United… November …         1997 R     \n 9 s9      Movie   706   Shravan K… Divy… India   April 1, …         2019 TV-14 \n10 s10     Movie   1920  Vikram Bh… Rajn… India   December …         2008 TV-MA \n# ℹ 7,777 more rows\n# ℹ 3 more variables: duration &lt;chr&gt;, listed_in &lt;chr&gt;, description &lt;chr&gt;\n\n\n\nlibrary(dplyr)\nlibrary(tidytext)\nlibrary(tidyr)\nlibrary(stringr)\nlibrary(ggplot2)\n\n\nstop_words &lt;- c(\"the\", \"a\", \"of\", \"and\", \"in\", \"on\", \"with\", \"is\", \"to\", \"for\", \"at\", \"s\", \"2\", \"i\")\n\ncommon_words &lt;- netflix |&gt;\n  mutate(title = str_to_lower(title),                               \n         title = str_replace_all(title, \"\\\\p{P}\", \" \")) |&gt;     \n  rowwise() |&gt;     #Process each row of the dataset separately, allowing for operations like splitting words in each title.                                                  \n  mutate(words = str_split(title, \"\\\\s+\")) |&gt; # split words separated by spaces             \n  unnest(words) |&gt;      # Each word from a title becomes its own row.             \n  filter(!words %in% stop_words, words != \"\") |&gt;                                            \n  group_by(words) |&gt;                                      \n  mutate(n = n()) |&gt;                                            \n  arrange(desc(n)) |&gt;                                          \n  distinct(words, n, .keep_all = TRUE) |&gt;\n  select(words, n) |&gt;\n  head(10)\n\ncommon_words\n\n# A tibble: 10 × 2\n# Groups:   words [10]\n   words         n\n   &lt;chr&gt;     &lt;int&gt;\n 1 love        152\n 2 my          127\n 3 you          81\n 4 man          79\n 5 christmas    78\n 6 world        69\n 7 story        67\n 8 life         66\n 9 movie        60\n10 little       58\n\n\n\nggplot(common_words, aes(x = reorder(words, -n),  y = n)) + \n  geom_bar(stat = \"identity\", fill = \"pink\") +\n  labs(title = \"Most Common Words in Netflix Titles\", \n       x = \"Words\", \n       y = \"Frequency\") +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1)) \n\n\n\n\n\n\n\n\nThis plot shows the most common occurring words in Netflix titles if you remove stock words such as “the”, “and” and “of”. Some obvious ‘buzzwords’ include “love,”story” and “movie”. Some key words that surprised me include “Christmas”, but it also makes sense as Christmas movies, much like songs, are very popular at that specific time of year. “Man” is also an interesting observation considering there is no appearance of “woman” in the top 10 words.\n\nnetflix_by_year &lt;- netflix |&gt;\n  group_by(release_year, type) |&gt;\n  summarise(count = n())  \n\nnetflix_by_year\n\n# A tibble: 118 × 3\n# Groups:   release_year [73]\n   release_year type    count\n          &lt;dbl&gt; &lt;chr&gt;   &lt;int&gt;\n 1         1925 TV Show     1\n 2         1942 Movie       2\n 3         1943 Movie       3\n 4         1944 Movie       3\n 5         1945 Movie       3\n 6         1946 Movie       1\n 7         1946 TV Show     1\n 8         1947 Movie       1\n 9         1954 Movie       2\n10         1955 Movie       3\n# ℹ 108 more rows\n\nggplot(netflix_by_year, aes(x = release_year, y = count, color = type)) +\n  geom_line() +\n  labs(title = \"Number of Movies and TV Shows Released by Year on Netflix\", \n       x = \"Release Year\", \n       y = \"Count\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nThis graph shows the number of movies and TV shows and their corresponding release years. It shows the increase in number of movies as the year heads towards present day. It also demonstrates the disparity of movies vs. TV shows. And that there has always been more movies than TT shows on Netflix. An interesting observation is in 2021, where there is growth in the number of TV shows released in this year, and a decline in movies. It is also cool to note that the earliest title on Netflix was released in 1925, and it is a TV show!!\n\ntotal_titles &lt;- nrow(netflix)\n\nprop_titles &lt;- netflix |&gt;\n  mutate(title = str_to_lower(title)) |&gt; \n  summarise(\n    count_numbers = sum(str_detect(title, \"\\\\d\")),  \n    count_the_anywhere = sum(str_detect(title, \"the\")),  \n    count_the_start = sum(str_detect(title, \"^the\"))) |&gt;\n  mutate(\n    The_Anywhere = count_the_anywhere / total_titles, \n    Start_With_The = count_the_start / total_titles, \n    Digit_Anywhere = count_numbers / total_titles  \n  )\n\nprop_titles\n\n# A tibble: 1 × 6\n  count_numbers count_the_anywhere count_the_start The_Anywhere Start_With_The\n          &lt;int&gt;              &lt;int&gt;           &lt;int&gt;        &lt;dbl&gt;          &lt;dbl&gt;\n1           426               1898             997        0.244          0.128\n# ℹ 1 more variable: Digit_Anywhere &lt;dbl&gt;\n\nprop_titles_long &lt;- prop_titles |&gt;\n  pivot_longer(\n    cols = c(The_Anywhere, Start_With_The, Digit_Anywhere),  \n    names_to = \"condition\",              \n    values_to = \"proportion\"             \n  ) |&gt;\n  select(condition, proportion)\n\nprop_titles_long \n\n# A tibble: 3 × 2\n  condition      proportion\n  &lt;chr&gt;               &lt;dbl&gt;\n1 The_Anywhere       0.244 \n2 Start_With_The     0.128 \n3 Digit_Anywhere     0.0547\n\nggplot(prop_titles_long, aes(x = condition, y = proportion, fill = condition)) +\n  geom_bar(stat = \"identity\") +\n  labs(\n    title = \"Proportions of Words in Netflix Titles\",\n    x = \"Title Condition\",\n    y = \"Proportion\",\n    fill = \"Condition\"\n  ) +\n  scale_y_continuous(labels = scales::percent) +\n  scale_x_discrete(labels = c(\n    The_Anywhere = '\"The\" anywhere',\n    Start_With_The = 'Start with \"The\"',\n    Digit_Anywhere = '\"Digit\" anywhere')) +\n  scale_fill_discrete(labels = c(\n    The_Anywhere = '\"The\" anywhere',\n    Start_With_The = 'Start with \"The\"',\n    Digit_Anywhere = '\"Digit\" anywhere')) +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n\n\n\n\n\n\n\nThis plot shows the proportions of titles that start with “the”, contain “the” or contain any digit. Almost exactly twice as many titles contain “the” anywhere in the title than titles that start with “the”. Almost a quarter of Netflix titles contain the word “the”. Far fewer titles contain digits, at only 5.5%. I expected this proportion to be higher."
  },
  {
    "objectID": "Presentation.html#project-1---part-1---goals-scored-at-each-world-cup",
    "href": "Presentation.html#project-1---part-1---goals-scored-at-each-world-cup",
    "title": "Sam’s Website Presentation",
    "section": "Project 1 - Part 1 - Goals Scored at Each World Cup",
    "text": "Project 1 - Part 1 - Goals Scored at Each World Cup\nThis analysis involved a dataset in tidytuesday that fascinated me as a soccer enthusiast, titled “World Cup”. The data included information on results, location, matches played and goals scored between 1930 and 2018, the most recent World Cup when the dataset was created.\nI chose to visualize the number of goals scored at each tournament dating back to 1930."
  },
  {
    "objectID": "Presentation.html#visualization",
    "href": "Presentation.html#visualization",
    "title": "Sam’s Website Presentation",
    "section": "Visualization",
    "text": "Visualization\n\n\n# A tibble: 21 × 2\n# Groups:   year [21]\n    year goals_scored\n   &lt;dbl&gt;        &lt;dbl&gt;\n 1  1930           70\n 2  1934           70\n 3  1938           84\n 4  1950           88\n 5  1954          140\n 6  1958          126\n 7  1962           89\n 8  1966           89\n 9  1970           95\n10  1974           97\n# ℹ 11 more rows"
  },
  {
    "objectID": "Presentation.html#project-1---part-2---chocolate-ratngs-by-percent-cocoa",
    "href": "Presentation.html#project-1---part-2---chocolate-ratngs-by-percent-cocoa",
    "title": "Sam’s Website Presentation",
    "section": "Project 1 - Part 2 - Chocolate Ratngs by Percent Cocoa",
    "text": "Project 1 - Part 2 - Chocolate Ratngs by Percent Cocoa\nIn this analysis, I chose another dataset that intrigued me titled “Chocolate Ratings”. I visualized the data with an aim to find the optimal percent of cocoa in a chocolate bar, in terms of ratings."
  },
  {
    "objectID": "Presentation.html#visualization-1",
    "href": "Presentation.html#visualization-1",
    "title": "Sam’s Website Presentation",
    "section": "Visualization",
    "text": "Visualization\nHere is the piped data that I used. I had to convert percentages to numerics so I could order the data correctly.\n\nchocolate |&gt;\n  mutate(cocoa_percent = as.numeric(sub(\"%\", \"\", cocoa_percent))) |&gt;\n  select(cocoa_percent, rating) |&gt;\n  group_by(cocoa_percent) |&gt;\n  arrange(cocoa_percent) |&gt;\n  summarise(ave_rating = mean(rating))\n\n# A tibble: 46 × 2\n   cocoa_percent ave_rating\n           &lt;dbl&gt;      &lt;dbl&gt;\n 1          42         2.75\n 2          46         2.75\n 3          50         3.75\n 4          53         2   \n 5          55         2.86\n 6          56         3.25\n 7          57         2.75\n 8          58         3.12\n 9          60         3.01\n10          60.5       2.75\n# ℹ 36 more rows"
  },
  {
    "objectID": "Presentation.html#project-2---netflix-title-analysis",
    "href": "Presentation.html#project-2---netflix-title-analysis",
    "title": "Sam’s Website Presentation",
    "section": "Project 2 - Netflix Title Analysis",
    "text": "Project 2 - Netflix Title Analysis\nIn this project, I focused on data involving Netflix titles and made three visualizations. The purpose of this project was to use piping to organize data, and focus on the use of regular expressions to assist in this process, so it is easier to understand and present."
  },
  {
    "objectID": "Presentation.html#visualization-1-1",
    "href": "Presentation.html#visualization-1-1",
    "title": "Sam’s Website Presentation",
    "section": "Visualization 1",
    "text": "Visualization 1\nFirstly, I wanted to find the most common words in titles of Netflix shows and TV shows, excluding filler words like “the” and “and”. This is the organized dataset that was used for my visualization.\n\n\n# A tibble: 10 × 2\n# Groups:   words [10]\n   words         n\n   &lt;chr&gt;     &lt;int&gt;\n 1 love        152\n 2 my          127\n 3 you          81\n 4 man          79\n 5 christmas    78\n 6 world        69\n 7 story        67\n 8 life         66\n 9 movie        60\n10 little       58"
  },
  {
    "objectID": "Presentation.html#visualization-2",
    "href": "Presentation.html#visualization-2",
    "title": "Sam’s Website Presentation",
    "section": "Visualization 2",
    "text": "Visualization 2\nSecondly, I compared the number of titles on Netflix that were movies or TV Shows, and their release dates.\n\n\n# A tibble: 118 × 3\n# Groups:   release_year [73]\n   release_year type    count\n          &lt;dbl&gt; &lt;chr&gt;   &lt;int&gt;\n 1         1925 TV Show     1\n 2         1942 Movie       2\n 3         1943 Movie       3\n 4         1944 Movie       3\n 5         1945 Movie       3\n 6         1946 Movie       1\n 7         1946 TV Show     1\n 8         1947 Movie       1\n 9         1954 Movie       2\n10         1955 Movie       3\n# ℹ 108 more rows"
  },
  {
    "objectID": "Presentation.html#visualization-3",
    "href": "Presentation.html#visualization-3",
    "title": "Sam’s Website Presentation",
    "section": "Visualization 3",
    "text": "Visualization 3\nFinally, I decided to find the percent of titles that either contain a digit anywhere in their title, start with “the” or have the word “the” anywhere and showed this information on a bar graph. This turned out to be far more difficult than I expected. I found it hard to name the axis labels separate from the variable names, which were sometimes confusing, as you can imagine."
  },
  {
    "objectID": "Presentation.html#project-3---generational-marijuana-use",
    "href": "Presentation.html#project-3---generational-marijuana-use",
    "title": "Sam’s Website Presentation",
    "section": "Project 3 - Generational Marijuana Use",
    "text": "Project 3 - Generational Marijuana Use\nThis project involved running permutation tests assuming a null hypothesis to test whether a relationship exists between two variables. In my analysis, I tested the relationship between parental and child use of marijuana. This was my favorite project. I really enjoyed running the statistical analysis and demonstrating the data."
  },
  {
    "objectID": "Presentation.html#visualization-4",
    "href": "Presentation.html#visualization-4",
    "title": "Sam’s Website Presentation",
    "section": "Visualization",
    "text": "Visualization\nI first found the percentage difference between students’ use of marijuana if their parents’ used it vs. if they didn’t. There is a 19.5% higher chance that a student uses marijuana if their parents’ did.\n\n\n# A tibble: 445 × 2\n   student parents\n   &lt;fct&gt;   &lt;fct&gt;  \n 1 uses    used   \n 2 uses    used   \n 3 uses    used   \n 4 uses    used   \n 5 uses    used   \n 6 uses    used   \n 7 uses    used   \n 8 uses    used   \n 9 uses    used   \n10 uses    used   \n# ℹ 435 more rows\n\n\n[1] 0.1952381\n\n\nThen I ran 1000 permutation tests to find the statistical likelihood of this happening without a relationship. i.e. assuming the null hypothesis of no relationship. On this graph you can see the results of the analysis, where the red line represents the proportional difference in the actual data.\n\n\n\n\n\n\n\n\n\nUsing this data I found a p-value of 0."
  },
  {
    "objectID": "Presentation.html#project-4---sql-analysis-of-wai-data-for-auditory-research",
    "href": "Presentation.html#project-4---sql-analysis-of-wai-data-for-auditory-research",
    "title": "Sam’s Website Presentation",
    "section": "Project 4 - SQL Analysis of WAI Data for Auditory Research",
    "text": "Project 4 - SQL Analysis of WAI Data for Auditory Research\nIn this project, I aimed to recreate a graph that demonstrates the relationship between the mean absorbance of sound and the frequency at which it is played. I then made my own graph, comparing mean absorbance across frequencies for people who identify as males, females or unknown sexes in studies conducted by Abur in 2004."
  },
  {
    "objectID": "Presentation.html#visualization-1-2",
    "href": "Presentation.html#visualization-1-2",
    "title": "Sam’s Website Presentation",
    "section": "Visualization 1",
    "text": "Visualization 1\nHere is my attempt at recreating the graph shown at https://pmc.ncbi.nlm.nih.gov/articles/PMC7093226/#F1.\n\nSELECT \n  Measurements.Identifier,\n  COUNT(DISTINCT CONCAT(Measurements.SubjectNumber, Measurements.Ear)) AS Unique_Ears,\n  PI_Info.AuthorsShortList,\n  Measurements.Instrument,\n  Measurements.Frequency,\n  AVG(Measurements.Absorbance) AS MeanAbsorbance,\n  CONCAT(PI_Info.AuthorsShortList, ' et al. N=', \n         COUNT(DISTINCT CONCAT(Measurements.SubjectNumber, Measurements.Ear)), ', ', Measurements.Instrument) AS LegendLabel\nFROM Measurements\nJOIN PI_Info ON Measurements.Identifier = PI_Info.Identifier\nWHERE Measurements.Identifier IN ('Abur_2014', 'Feeney_207', 'Groon_2015', 'Lewis_2015', 'Liu_2008', 'Rosowski_2012', 'Shahnaz_2006', 'Shaver_2013', 'Sun_2016', 'Voss_1994', 'Voss_2010', 'Werner_2010')\n  AND Measurements.Frequency &gt;= 200\nGROUP BY Measurements.Identifier, Measurements.Instrument, PI_Info.AuthorsShortList, Measurements.Frequency;"
  },
  {
    "objectID": "Presentation.html#visualization-2-1",
    "href": "Presentation.html#visualization-2-1",
    "title": "Sam’s Website Presentation",
    "section": "Visualization 2",
    "text": "Visualization 2\nIn my second visualization I compare the mean absorbances across the study conducted by Abur in 2004. I differentiate between Men, Women and the studies where the sex of the subject was unknown."
  },
  {
    "objectID": "Presentation.html#project-1---part-2---chocolate-ratings-by-percent-cocoa",
    "href": "Presentation.html#project-1---part-2---chocolate-ratings-by-percent-cocoa",
    "title": "Sam’s Website Presentation",
    "section": "Project 1 - Part 2 - Chocolate Ratings by Percent Cocoa",
    "text": "Project 1 - Part 2 - Chocolate Ratings by Percent Cocoa\nIn this analysis, I chose another dataset that intrigued me, titled “Chocolate Ratings”. I visualized the data with an aim to find the optimal percent of cocoa in a chocolate bar, in terms of ratings."
  },
  {
    "objectID": "Presentation.html#thank-you",
    "href": "Presentation.html#thank-you",
    "title": "Sam’s Website Presentation",
    "section": "Thank You",
    "text": "Thank You"
  },
  {
    "objectID": "Presentation.html#thank-youp",
    "href": "Presentation.html#thank-youp",
    "title": "Sam’s Website Presentation",
    "section": "Thank Youp",
    "text": "Thank Youp"
  }
]